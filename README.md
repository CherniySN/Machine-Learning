# CRISP-DM Cross-Indastry Standart Proces for Data Mining или Межотраслевой стандартный процесс интелектуального анализа данных.
![Каритинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/CRISP-DM_Process_Diagram%20(1).png)

Согласно CRISP-DM (указан выше на рисунке) машинное обучение состоит из шести этапов:
1. Бизнес-анализ
2. Анализ данных
3. Подготовка данных
4. Моделирование
5. Оценка
6. Развертывание

Задачи которые стоит решить на каждом этапе:
- На этапе бизнес анализа мы пытаемся конкретизировать задачу или проблему и понять как можно решить данную проблему;
- На этапе анализа данных мы анализируем доступные коллекции данных и решаем достаточно ли нам этих данных;
- На этапе подготовки данных мы занимаемся преобразованием данных;
- На этапе моделирования мы обуаем модель;
- После оцениваем лучшую модель и оцениваем ее успешность в решении поставленной задачи;
- Этап развертывания - развертывание модели в продакшене.
![Каритинка](![Методологи по пунктам](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/CRISP%20-%20DM.jpg)

# Моделирование и выбор модели.
Онлайн-тестирование - это развертывание модели и оценка качества на реальных данных.

В процесси моделирования мы можем сэмитировать запуск модеи и проверки ее, разделив коллекцию данных на обучающую и проверочну.
(![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%94%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.jpg)

Как видно из картинки мы взяли 90% для обчение и 10% для проверки модели. 

И так мы предпологаем, что данную задачу хорошо решает линейная регрессия и нейронная сеть и нам теперь стоит определить, какая из них лучше решает задачу на наших данных.
![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.jpg)

Но стоит помнить, что у нейронных сетей есть свои параметры, которыми мы можем ее настраивать, так же и у линей регрессии есть свои параметры. Обучим несколько вариантов моделей с разными параметрами и посмотрим на результат.
![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%92%D1%8B%D0%B1%D0%BE%D1%80%20%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2%20%D0%B8%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5.jpg)

#### ВНИМАНИЕ!!!
Если мы раз за разом проводим оценку модели на проверочных данных, то высокая точность модели может оказаться случацностью! Т.е. нам просто может повезти с результатами.
В статистике эта проблема называется -"Проблема множественных сравнений". Чем больше раз мы оцениваем точность нашей модели тем больше вероятность того. что мы случайно увидим хорошую производительность.
#### РЕШЕНИЕ!!!
Что бы этого избежать мы поделим наши данные еще раз. Тепрь 80% - для обучения, 10% - проверочные данные, 10% - тестовые данные.
![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%92%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%82%D0%B5%D1%81%D1%82%D0%BE%D0%B2%D0%BE%D0%B9%20%D1%87%D0%B0%D1%82%D0%B8.jpg)

Дале мы проводим обучение, оцениваем работу моделей на проверочном наборе данных и ВЫБРАВ ЛУЧШУЮ МЫ ОЦЕНИМ ЕЕ НА ТЕСТОВОМ НАБОРЕ ДАННЫХ.
![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%A2%D0%B5%D1%81%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%BB%D1%83%D1%87%D1%88%D0%B5%D0%B9%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8.jpg)

Процесс выбора лучшей модели:
1. Делим данные на обучающую выборку, проверочную выборку и тестовую выборку.
2. Обучаем каждую модель на обучающей части и затем проверяем на проверочной.
3. Определяем лучшую модель и тестируем ее на тестовой выборке.
![Картинка](https://github.com/CherniySN/Machine-learning-process-CRISP-DM/blob/main/%D0%9F%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81%20%D0%B2%D1%8B%D0%B1%D0%BE%D1%80%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8.jpg)
Важно использовать этот процесс пока не найдем лучшую модель, убедившись что модель пригодна мы переходим к следующему шагу - развертывание.

# Задача линейной регрессии. Прогнозирование цены квартиры.

Предположим, что нам задали создать программу, которая оценивает стоимость недвижимость по некоторым ее параметрам и нам дали коллекцию данных.
Для примера мы возмем коллекцию данных по недвижимости в Нью-Йорке с сайта Kaggel, ссыль ниже.

Объявления и показатели Airbnb в Нью-Йорке, Нью-Йорке, США (2019 г.): https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data

И так в дадатсете у нас есть район, широта, долгота, и описание обьекта недвижимости - это признаки или фичи, которые нам необходимо использовать для прогнозирования цены за обьект недвижимости. Так же в датасете у нас есть цена - это целевая переменная, которую мы и будем предсказывать.

И так это означает, что у нас будет **контролируемый тип обучения**, а полскольку целевая переменная у нас числовая, то это **задача регрессии**, если бы целевая переменная была бы котегориальной, то это была бы **задача класификации**.


#### План действий:
1. Загрузка датасета;
2. Разведочный анализ данных;
3. Выбор стратегии проверки;
4. Написание модели линейной регрессии;
5. Конструировоние признаков с целью улучшения нашей модели;
6. Использование регуляризации.

#### Загрузка датасета.

Тут все просто, с сайта https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data скачаем данные и распкуем их в папке нашего проекта. 

#### Разведочный анализ данных.

EDA (exploratory data analysis, EDA) - разведочный анализ данных. И так прежде чем скорее, скорее начать обучать нашу модель нам необходимо посмотреть, какие данные у нас есть и насколько они полезны. В этом пункте следуем следующему набору правил:

1. Изучение распределения целеовой переменной
2. Изучение признаков в наборе данных
3. Распределение значений признаков
4. Наличие пропущенных значений
5. Качество данных

# Градиентный спуск.

У аналитического решения задачи расчёта коэффициентов регрессии есть ряд недостатков, в том числе:

1. вычислительная сложность из-за матричного перемножения. При увеличении размерности матрицы в 10 раз сложность вычислений увеличивается в 1000 раз;
2. неустойчивость вычислений: при попытке найти обратную матрицу, которая может не существовать, в алгоритме нахождения обратной матрицы возникает деление на ноль.

Избежать этих проблем можно с помощью приближённых численных методов. При этом не надо будет перемножать матрицы или находить обратные матрицы. Самый простой и эффективный из этих методов называется методом градиентного спуска. Суть метода состоит в обновлении параметров модели 𝑤 по маленьким шажкам вместо того, чтобы находить их сразу. Каждый такой шажок называется итерацией.

**Когда следует использовать градиентный спуск** 
* Когда данных очень много - нехватит памяти у компьютера;
* Когда нужно контролировать точность обучения;
  
**Когда не стоит применять градиентный спукс**
* Когда данных мало. В этом случае лучше использовать Ridge или Lasso.

sklearn.linear_model.SGDRegressor — класс библиотеки sklearn, в котором реализован градиентный спуск. Класс принимает следующие параметры:

1. learning_rate='constant' — используем самую простую модификацию спуска из нескольких возможных (см. документацию);
2. eta0 — шаг градиентного спуска;
3. fit_intercept — даёт возможность обучить коэффициент при свободном члене линейной регрессии;
4. random_state — этот параметр нужен для воспроизводимости вычислений.

Метод partial_fit() делает одну итерацию градиентного спуска.



